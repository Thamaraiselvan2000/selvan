{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f60a3c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: \n",
      " ['daughter' 'had' 'have' 'lamp' 'lamps' 'little' 'mary' 'of' 'silence'\n",
      " 'starthe' 'twinkle']\n",
      "\n",
      "BOW Matrix: \n",
      "\n",
      "[[1 0 1 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 1 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text=['I have a little daughter', 'Mary had a little lamp', 'Twinkle twinkle little star' 'The silence of lamps']\n",
    "\n",
    "#Initialize the vector\n",
    "vectorizer=CountVectorizer()\n",
    "\n",
    "#transform\n",
    "x=vectorizer.fit_transform(text)\n",
    "\n",
    "#get feature names (words)\n",
    "\n",
    "feature_names=vectorizer.get_feature_names_out()\n",
    "\n",
    "#Display BOW-Matrix\n",
    "\n",
    "print('Feature names: \\n', feature_names)\n",
    "print()\n",
    "print('BOW Matrix: \\n')\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd964dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names(TF-IDF):\n",
      " ['daughter' 'had' 'have' 'lamp' 'lamps' 'little' 'mary' 'of' 'silence'\n",
      " 'star' 'the' 'twinkle']\n",
      "\n",
      "TF-IDF Matrix:\n",
      " [[0.64450299 0.         0.64450299 0.         0.         0.41137791\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.5417361  0.         0.5417361  0.         0.34578314\n",
      "  0.5417361  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.27448674\n",
      "  0.         0.         0.         0.43003652 0.         0.86007303]\n",
      " [0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.5        0.5        0.         0.5        0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text=['I have a little daughter', 'Mary had a little lamp', 'Twinkle twinkle little star','The silence of lamps']\n",
    "\n",
    "tfidf=TfidfVectorizer()\n",
    "x_tfidf=tfidf.fit_transform(text)\n",
    "\n",
    "features_names_tfidf=tfidf.get_feature_names_out()\n",
    "\n",
    "print('Feature Names(TF-IDF):\\n', features_names_tfidf)\n",
    "print()\n",
    "print('TF-IDF Matrix:\\n', x_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0620d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a statement: \n",
      "\tAlice opened the door and found that it led into a small passage, not much larger than a rat-hole: she knelt down and looked along the passage into the loveliest garden you ever saw.\n",
      "Sentences: \n",
      " ['Alice opened the door and found that it led into a small passage, not much larger than a rat-hole: she knelt down and looked along the passage into the loveliest garden you ever saw.']\n",
      "Alice opened the door and found that it led into a small passage, not much larger than a rat-hole: she knelt down and looked along the passage into the loveliest garden you ever saw.\n",
      "182\n",
      "\n",
      "Feature Names(TF-IDF):\n",
      " ['alice' 'along' 'and' 'door' 'down' 'ever' 'found' 'garden' 'hole' 'into'\n",
      " 'it' 'knelt' 'larger' 'led' 'looked' 'loveliest' 'much' 'not' 'opened'\n",
      " 'passage' 'rat' 'saw' 'she' 'small' 'than' 'that' 'the' 'you']\n",
      "\n",
      "TF-IDF Matrix:\n",
      " [[0.1490712 0.1490712 0.2981424 0.1490712 0.1490712 0.1490712 0.1490712\n",
      "  0.1490712 0.1490712 0.2981424 0.1490712 0.1490712 0.1490712 0.1490712\n",
      "  0.1490712 0.1490712 0.1490712 0.1490712 0.1490712 0.2981424 0.1490712\n",
      "  0.1490712 0.1490712 0.1490712 0.1490712 0.1490712 0.4472136 0.1490712]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text=input('Enter a statement: \\n\\t')\n",
    "sent_token=sent_tokenize(text)\n",
    "print('Sentences: \\n', sent_token)\n",
    "\n",
    "for sent in sent_token:\n",
    "    print(sent)\n",
    "    print(len(sent))\n",
    "print()\n",
    "\n",
    "tfidf=TfidfVectorizer()\n",
    "x_tfidf=tfidf.fit_transform(sent_token)\n",
    "\n",
    "features_names_tfidf=tfidf.get_feature_names_out()\n",
    "\n",
    "print('Feature Names(TF-IDF):\\n',features_names_tfidf)\n",
    "print()\n",
    "print('TF-IDF Matrix:\\n', x_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6ec3280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: It is used in natural language processing\n",
      "\n",
      "Generated 2-grams:\n",
      "('It', 'is')\n",
      "('is', 'used')\n",
      "('used', 'in')\n",
      "('in', 'natural')\n",
      "('natural', 'language')\n",
      "('language', 'processing')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "\n",
    "text='It is used in natural language processing'\n",
    "tokens=nltk.word_tokenize(text)\n",
    "\n",
    "#Generate bigrams\n",
    "\n",
    "n=2\n",
    "bigrams=list(ngrams(tokens,n))\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print()\n",
    "print(f\"Generated {n}-grams:\") \n",
    "for gram in bigrams:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18eaf667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: It is used in natural language processing\n",
      "\n",
      "Generated 3-grams:\n",
      "('It', 'is', 'used')\n",
      "('is', 'used', 'in')\n",
      "('used', 'in', 'natural')\n",
      "('in', 'natural', 'language')\n",
      "('natural', 'language', 'processing')\n"
     ]
    }
   ],
   "source": [
    "#Generate bigrams\n",
    "\n",
    "n=3\n",
    "bigrams=list(ngrams(tokens,n))\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print()\n",
    "print(f\"Generated {n}-grams:\") \n",
    "for gram in bigrams:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b82f1d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: It is used in natural language processing\n",
      "\n",
      "Generated 4-grams:\n",
      "('It', 'is', 'used', 'in')\n",
      "('is', 'used', 'in', 'natural')\n",
      "('used', 'in', 'natural', 'language')\n",
      "('in', 'natural', 'language', 'processing')\n"
     ]
    }
   ],
   "source": [
    "#Generate bigrams\n",
    "\n",
    "n=4\n",
    "bigrams=list(ngrams(tokens,n))\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print()\n",
    "print(f\"Generated {n}-grams:\") \n",
    "for gram in bigrams:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6b9068d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])]\n",
      "[array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "text=['I have a little daughter', 'Mary had a little lamp', 'Twinkle twinkle little star','The silence of lamps']\n",
    "\n",
    "#step 1- tokens\n",
    "tokens=[word for sent in text for word in sent.lower().split()]\n",
    "\n",
    "#step 2- vocabulary\n",
    "vocabulary=list(set(tokens))# unique words in the text\n",
    "\n",
    "\n",
    "#initialize encoder\n",
    "encoder=OneHotEncoder(categories=[vocabulary], sparse=False)\n",
    "\n",
    "#Perform the one-hot encoding\n",
    "\n",
    "one_hot_encoded=[]\n",
    "for sent in text:\n",
    "    sent_encoded=[]\n",
    "    for word in sent.lower().split():\n",
    "        word_index=vocabulary.index(word)\n",
    "        word_vector=np.zeros(len(vocabulary))\n",
    "        word_vector[word_index]=1\n",
    "        sent_encoded.append(word_vector)\n",
    "    one_hot_encoded.append(sent_encoded)\n",
    "\n",
    "for sent in one_hot_encoded:\n",
    "    print(sent)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e839ed7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c324c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c81235ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63cd9133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99aad6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\thama\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\thama\\anaconda3\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\thama\\anaconda3\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\thama\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim)\n",
      "  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\thama\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "     ---------------------------------------- 0.0/67.1 kB ? eta -:--:--\n",
      "     ------------------------ --------------- 41.0/67.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.1/67.1 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\thama\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\thama\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Collecting simpful (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Obtaining dependency information for simpful from https://files.pythonhosted.org/packages/8d/93/8448d3f1aa9d2911b8cba2602aaa1af85eb31a26d28b7b737f1fa5b40c02/simpful-2.11.1-py3-none-any.whl.metadata\n",
      "  Downloading simpful-2.11.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting fst-pso (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thama\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Collecting miniful (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Downloading simpful-2.11.1-py3-none-any.whl (32 kB)\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py): started\n",
      "  Building wheel for fst-pso (setup.py): finished with status 'done'\n",
      "  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20448 sha256=1e0edad13aa515d8e4d20dce88305132807adce19f0c8069155b956d3e80e59b\n",
      "  Stored in directory: c:\\users\\thama\\appdata\\local\\pip\\cache\\wheels\\69\\f5\\e5\\18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
      "  Building wheel for miniful (setup.py): started\n",
      "  Building wheel for miniful (setup.py): finished with status 'done'\n",
      "  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3522 sha256=5f6952bf1e11a7faab0da571f05c570fef0661b29b9597b7d487f22699765db4\n",
      "  Stored in directory: c:\\users\\thama\\appdata\\local\\pip\\cache\\wheels\\9d\\ff\\2f\\afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: simpful, miniful, fst-pso, pyfume, FuzzyTM\n",
      "Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 miniful-0.0.6 pyfume-0.2.25 simpful-2.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33ae4368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'semantic': [-0.04309844  0.01832869  0.02594942  0.02870969  0.03733459 -0.03083838\n",
      "  0.00552807  0.03023641 -0.01420025 -0.03086761 -0.00205112 -0.04184474\n",
      " -0.02800006  0.03552269  0.0167627   0.03612835  0.03400124  0.03765371\n",
      " -0.01894577 -0.00280903]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Vector for 'nlp': [-0.02797029  0.00865183 -0.00448686  0.03396846  0.01986795  0.02264736\n",
      "  0.00717153 -0.01349928 -0.02183406 -0.00516037  0.00718514 -0.01323004\n",
      " -0.03536891 -0.03902654 -0.04560893 -0.02967585 -0.00923712 -0.02161936\n",
      " -0.03230335 -0.01858661]\n",
      "====================================================================================================\n",
      "Similarity b/w 'word' and 'embeddings': 0.05595846474170685\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "#sample text\n",
    "\n",
    "text=['Word embeddings capture semantic relationships',\n",
    "      'Word2vec is a popular technique in nlp',\n",
    "      'Word embedding model in a continuos vector space']\n",
    "tokenized_text=[word_tokenize(sentence.lower()) for sentence in text]\n",
    "\n",
    "#Train Word2vec model\n",
    "model=Word2Vec(sentences=tokenized_text, vector_size=20,window=5,min_count=1, workers=4)\n",
    "\n",
    "#Find word vectors\n",
    "vector_semantic=model.wv['semantic']\n",
    "vector_nlp=model.wv['nlp']\n",
    "\n",
    "#similarity b/w words\n",
    "\n",
    "similarity=model.wv.similarity('word', 'embeddings')\n",
    "\n",
    "print(f\"Vector for 'semantic': {vector_semantic}\")\n",
    "print('-'*100)\n",
    "print(f\"Vector for 'nlp': {vector_nlp}\")\n",
    "print('='*100)\n",
    "print(f\"Similarity b/w 'word' and 'embeddings': {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2538d5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['word', 'embeddings', 'capture', 'semantic', 'relationships'], tags=['0']), TaggedDocument(words=['word2vec', 'is', 'a', 'popular', 'technique', 'in', 'nlp'], tags=['1']), TaggedDocument(words=['word', 'embedding', 'model', 'in', 'a', 'continuos', 'vector', 'space'], tags=['2'])]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#sample text\n",
    "\n",
    "documents=['Word embeddings capture semantic relationships',\n",
    "           'Word2vec is a popular technique in nlp',\n",
    "           'Word embedding model in a continuos vector space']\n",
    "\n",
    "#Tokenize & tag documents\n",
    "\n",
    "tagged_data=[TaggedDocument(words=word_tokenize (doc.lower()), tags=[str(i)]) for i, doc in enumerate(documents)]\n",
    "print(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cfdf0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Doc2Vec(vector_size=100,window=2,min_count=1,workers=5, epochs=20)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count,epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c930c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_doc_1=model.infer_vector(word_tokenize('Word embeddings capture semantic relationships',\n",
    "                                              'Word2vec is a popular technique in nlp',\n",
    "                                              'Word embedding model in a continuos vector space'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5765735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.6035238e-03, -1.9890747e-03,  4.3723667e-03,  2.1197842e-03,\n",
       "       -6.7712725e-05, -2.5892216e-03, -3.6983327e-03,  3.0281511e-03,\n",
       "        4.7519491e-03, -4.5778174e-03, -1.3389792e-03,  1.9155790e-05,\n",
       "        2.6954620e-03, -4.2717978e-03, -1.3793741e-03, -2.0585854e-03,\n",
       "       -1.1760993e-03, -2.1583568e-03,  9.9848374e-04, -3.0819490e-03,\n",
       "        1.9066416e-03, -3.2890358e-03,  4.2215837e-03,  3.7146220e-03,\n",
       "        4.1005143e-04, -2.7608136e-03, -2.8819701e-04, -2.0332194e-03,\n",
       "       -1.3203814e-03,  1.9238092e-03,  1.3767395e-04, -4.3475330e-03,\n",
       "        4.6175951e-03,  1.3399654e-03,  2.4797942e-03,  4.4075740e-03,\n",
       "        2.1847708e-03,  6.8986526e-05,  3.5275938e-03,  7.6590572e-04,\n",
       "       -2.4242576e-03, -4.4040424e-03, -4.1932845e-03, -3.0207334e-03,\n",
       "        4.0450739e-03,  1.0521623e-03, -4.3028896e-03, -1.5824392e-03,\n",
       "       -2.1688592e-04, -3.3605201e-03, -1.9021380e-03,  1.5187864e-03,\n",
       "       -4.7570956e-03,  3.3617199e-03, -3.3516674e-03, -3.1365503e-03,\n",
       "        3.0580475e-03,  7.0992764e-04,  3.1076751e-03,  8.8644918e-04,\n",
       "        4.2397375e-03,  4.1633956e-03, -4.4399328e-03, -3.6939771e-03,\n",
       "       -3.7459537e-04, -4.6856282e-03,  1.4779122e-03,  2.2614794e-03,\n",
       "       -3.8436917e-03, -4.0791323e-04, -4.1885716e-03,  4.6851872e-03,\n",
       "       -2.1984049e-03,  3.8314034e-04,  1.5669288e-03,  8.4739097e-04,\n",
       "        2.9952039e-03,  3.6275650e-03,  2.0679769e-03, -3.9572818e-03,\n",
       "       -3.7336145e-03, -2.3364833e-04,  1.8947095e-03,  3.2598835e-03,\n",
       "        2.7110048e-03,  9.6602389e-04, -2.2481603e-03,  1.7498467e-03,\n",
       "       -3.0528673e-04,  4.8930072e-03, -3.6779901e-03, -4.8803710e-03,\n",
       "        4.0120729e-03,  2.8888304e-03, -2.1845284e-03, -2.6258808e-03,\n",
       "       -3.7763049e-04,  2.0107813e-03,  3.9114407e-03, -3.1320143e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_doc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "659db806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector for 'Word embeddings capture semantic relationships': [-3.6035238e-03 -1.9890747e-03  4.3723667e-03  2.1197842e-03\n",
      " -6.7712725e-05 -2.5892216e-03 -3.6983327e-03  3.0281511e-03\n",
      "  4.7519491e-03 -4.5778174e-03 -1.3389792e-03  1.9155790e-05\n",
      "  2.6954620e-03 -4.2717978e-03 -1.3793741e-03 -2.0585854e-03\n",
      " -1.1760993e-03 -2.1583568e-03  9.9848374e-04 -3.0819490e-03\n",
      "  1.9066416e-03 -3.2890358e-03  4.2215837e-03  3.7146220e-03\n",
      "  4.1005143e-04 -2.7608136e-03 -2.8819701e-04 -2.0332194e-03\n",
      " -1.3203814e-03  1.9238092e-03  1.3767395e-04 -4.3475330e-03\n",
      "  4.6175951e-03  1.3399654e-03  2.4797942e-03  4.4075740e-03\n",
      "  2.1847708e-03  6.8986526e-05  3.5275938e-03  7.6590572e-04\n",
      " -2.4242576e-03 -4.4040424e-03 -4.1932845e-03 -3.0207334e-03\n",
      "  4.0450739e-03  1.0521623e-03 -4.3028896e-03 -1.5824392e-03\n",
      " -2.1688592e-04 -3.3605201e-03 -1.9021380e-03  1.5187864e-03\n",
      " -4.7570956e-03  3.3617199e-03 -3.3516674e-03 -3.1365503e-03\n",
      "  3.0580475e-03  7.0992764e-04  3.1076751e-03  8.8644918e-04\n",
      "  4.2397375e-03  4.1633956e-03 -4.4399328e-03 -3.6939771e-03\n",
      " -3.7459537e-04 -4.6856282e-03  1.4779122e-03  2.2614794e-03\n",
      " -3.8436917e-03 -4.0791323e-04 -4.1885716e-03  4.6851872e-03\n",
      " -2.1984049e-03  3.8314034e-04  1.5669288e-03  8.4739097e-04\n",
      "  2.9952039e-03  3.6275650e-03  2.0679769e-03 -3.9572818e-03\n",
      " -3.7336145e-03 -2.3364833e-04  1.8947095e-03  3.2598835e-03\n",
      "  2.7110048e-03  9.6602389e-04 -2.2481603e-03  1.7498467e-03\n",
      " -3.0528673e-04  4.8930072e-03 -3.6779901e-03 -4.8803710e-03\n",
      "  4.0120729e-03  2.8888304e-03 -2.1845284e-03 -2.6258808e-03\n",
      " -3.7763049e-04  2.0107813e-03  3.9114407e-03 -3.1320143e-03]\n",
      "\n",
      "Most similar document: [('2', 0.10934638231992722), ('1', -0.033330243080854416), ('0', -0.04397440701723099)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thama\\AppData\\Local\\Temp\\ipykernel_15480\\3566639832.py:3: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  similar_doc=model.docvecs.most_similar(positive=[vector_doc_1])\n"
     ]
    }
   ],
   "source": [
    "#find the most similar document\n",
    "\n",
    "similar_doc=model.docvecs.most_similar(positive=[vector_doc_1])\n",
    "\n",
    "print(f\"vector for 'Word embeddings capture semantic relationships': {vector_doc_1}\")\n",
    "print()\n",
    "print(f\"Most similar document: {similar_doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc95af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
